{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bslOTCx8bsonn8wZMt7VdZ_s8e7HCAXF","timestamp":1750692897608}],"authorship_tag":"ABX9TyPx1+jOjlirgr0Z3VwEla3R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Install required packages"],"metadata":{"id":"tD6flo0ia2sU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"00yXcHG6UJJ3"},"outputs":[],"source":["!pip install OpenAI"]},{"cell_type":"code","source":["!pip install pandas"],"metadata":{"id":"U34liBEtaUa2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install IPython"],"metadata":{"id":"2f4qYqTMaWxu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Import modules"],"metadata":{"id":"OJ25t6btbEsU"}},{"cell_type":"code","source":["import json, os\n","from openai import OpenAI\n","from datetime import datetime, timedelta\n","import pandas as pd\n","from tqdm import tqdm\n","from IPython.display import display, Markdown\n","from openai.types.chat import ChatCompletionMessageParam"],"metadata":{"id":"10ZyEgKTagFs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mount Google drive to access JSON file and CSV file"],"metadata":{"id":"QzWzHOPiZfhq"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"id":"h5EXxxP9jVem","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755742607575,"user_tz":360,"elapsed":643,"user":{"displayName":"John Coleman","userId":"08155288022400272840"}},"outputId":"28085ae7-f785-47f4-d2f4-658239c0564a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["Load API Key and initilize OpenAI client"],"metadata":{"id":"AtNi8m-fb2lz"}},{"cell_type":"code","source":["file_name = '/content/drive/MyDrive/MidTermFiles/config.json'\n","\n","with open(file_name, 'r') as file:\n","  config = json.load(file)\n","  os.environ['OPENAI_API_KEY'] = config.get(\"API_KEY\")\n","  os.environ[\"OPENAI_BASE_URL\"] = config.get(\"OPENAI_API_BASE\")"],"metadata":{"id":"qmJcdW08bI60"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#@title LLM Function\n","def llm(system_prompt, user_prompt, model_name=\"gpt-4o-mini\", temperature=0.0):\n","\n","    try:\n","\n","        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n","\n","        prompt: list[ChatCompletionMessageParam] = [\n","            {'role': 'system', 'content': system_prompt},\n","            {'role': 'user', 'content': user_prompt}\n","        ]\n","\n","        response = client.chat.completions.create(\n","            model=model_name,\n","            messages=prompt,\n","            temperature=temperature\n","        )\n","\n","        return response.choices[0].message.content\n","\n","    except Exception as e:\n","        error_message = f\"Sorry, I encountered the following error: {e}\"\n","        print(error_message)\n","        return error_message"],"metadata":{"id":"Z9lUOuwycU6S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Load Alex's Emails CSV File\n","\n","df = pd.read_csv(\"/content/drive/MyDrive/MidTermFiles/Alex_emails_march_04.csv\", index_col=\"email_id\", encoding='latin-1')\n","\n","yesterday_date = pd.to_datetime(\"3/3/2025\").strftime('%m/%d/%Y')\n","\n","df['date_received'] = pd.to_datetime(df['date_received']).dt.strftime('%m/%d/%Y')\n","\n","yesterday_emails = df[df['date_received'] == yesterday_date].reset_index(drop=True)\n","print(f\"Filtered Email Count: {len(yesterday_emails)}\")\n","\n","df.shape\n","\n","yesterday_emails.info()"],"metadata":{"id":"DgKO0y1dlYRE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Create Categories List\n","\n","categories = ['Urgent & High_Priority Emails',\n","              'Deadline-Driven Emails',\n","              'Routine Updates & Check-ins',\n","              'Non-Urgent Informational Emails',\n","              'Personal & Social Emails',\n","              'Spam/Unimportant Emails'\n","              ]"],"metadata":{"id":"9pOEiw_Znk7a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title System Prompt\n","\n","system_prompt = \"\"\"\n","\n","You are an expert AI assistant tasked with accurately categorizing emails into one of the following six categories:\n","\n","'Urgent & High_Priority Emails': Requires immediate attention, action, or response. Often contains keywords like \"urgent,\" \"important,\" \"action required,\" or has a demanding tone.\n","'Deadline-Driven Emails': Pertains to a specific deadline, submission, or time-sensitive task. Look for dates, times, or phrases like \"due by,\" \"deadline,\" or \"submit by.\"\n","'Routine Updates & Check-ins': Standard operational messages, status reports, meeting minutes, or simple check-ins that do not require an immediate or urgent response.\n","'Non-Urgent Informational Emails': General announcements, newsletters, articles, or FYI (For Your Information) content that is good to know but not actionable.\n","'Personal & Social Emails': Non-work-related messages from friends, family, or social event invitations.\n","'Spam/Unimportant Emails': Unsolicited junk mail, advertisements, or promotional content.\n","\n","Your response MUST be only the category name from the list above. Do not add any other text, explanations, or punctuation.\n","\n","Here are a few examples:\n","\n","---\n","Email:\n","Subject: [URGENT] Dashboard Syncing Issues – Production Metrics Missing\n","Body: We’ve got a big issue right now—live production metrics aren’t syncing properly on the Orion Analytics Dashboard. Some numbers are missing, others just look wrong. We’re seeing data gaps in yesterday’s batch reports, and my team can’t verify production outputs.\n","\n","Couple of things:\n","\n","This started sometime overnight. No one touched the system, but by 7 AM today, we noticed discrepancies.\n","Not clear if this is an API issue or data processing lag. Can your team confirm?\n","Production is running blind right now—we need this resolved ASAP or at least tell us how to get accurate numbers another way.\n","Need an update in the next 24 hours, or I’ll have to escalate further. If you need someone from our IT side, let me know.\n","Category: Urgent & High_Priority Emails\n","---\n","---\n","Email:\n","Subject: Approval Request: Budget Approval Needed by EOD\n","Body: I hope you're doing well. As we approach the deadline for finalizing the budget, I wanted to bring this to your attention. We need approval by the end of today to ensure smooth execution of next quarter's projects.\n","\n","Attached, you'll find the latest budget breakdown. Let me know if you have any concerns or if you'd like to discuss any of the allocations before we move forward. Your prompt approval will help us avoid unnecessary delays.\n","\n","Looking forward to your confirmation\n","Category: Deadline-Driven Emails\n","---\n","---\n","Email:\n","Subject: Daily Update – Project Titan (March 3)\n","Body: Completed Today:\n","Finished API integration for the reporting module. Data sync tests are successful with minor latency (~200ms).\n","Resolved the UI responsiveness issue on smaller screens (tested across devices).\n","Security team reviewed latest patches, no critical vulnerabilities found.\n","In Progress / Challenges:\n","Load Testing: Found performance dips when concurrent users exceed 5,000. Investigating if it's DB bottleneck or caching issue.\n","Client Feedback Implementation: We’re waiting on John (Acme Manufacturing) to clarify requirements before finalizing UI changes.\n","Bug #3421 (Session Timeouts): Still reproducing intermittently—Tom’s team is debugging with logs.\n","Next Steps (Tomorrow):\n","Start final API optimization based on today’s load testing results.\n","Deploy hotfix for session timeouts (if root cause is confirmed).\n","Prep for Friday’s internal review with the leadership team.\n","Let me know if you need any specific details or adjustments.\n","Category: Routine Updates & Check-ins\n","---\n","---\n","Email:\n","Subject: 10X Your Dev Team's Productivity with AI – Free Trial Ends Soon!\n","Body: Are your developers spending too much time debugging and fixing code instead of innovating? Our AI-powered coding assistant can write, optimize, and debug your code in seconds!\n","Limited-time offer: Get 3 months FREE if you sign up today!\n","Claim Your Free Trial Now\n","Category: Spam/Unimportant Emails\n","---\n","\"\"\""],"metadata":{"id":"8LL2KnSTcray"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Loop through and categorize emails\n","\n","if 'category' not in yesterday_emails.columns:\n","    yesterday_emails['category'] = None\n","\n","email_data = yesterday_emails.to_dict('records')\n","\n","results = []\n","\n","for row in tqdm(email_data, total=len(email_data), desc='Processing emails'):\n","\n","    user_prompt = f\"\"\"\n","    Please categorize the following email:\n","\n","    Sender: {row['sender']}\n","    Subject: {row['subject']}\n","    Date Received: {row['date_received']}\n","    Recipient: {row['main_recipient']}\n","    Email Body: {row['body']}\n","    \"\"\"\n","\n","    category_by_llm = llm(system_prompt, user_prompt)\n","\n","    row['category'] = category_by_llm if category_by_llm in categories else \"Categorization_Failed\"\n","    results.append(row)\n","\n","categorized_df = pd.DataFrame(results)\n","pd.set_option('display.max_rows', None)\n","display(categorized_df)"],"metadata":{"id":"Hxava5upoE_B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Task 1a - Executive Dashboard**"],"metadata":{"id":"yiINlhk4T_tb"}},{"cell_type":"code","source":["# @title System Prompt\n","\n","system_prompt = \"\"\"\n","\n","You are an expert AI assistant tasked with creating a high-level executive summary from a provided list of emails. Your response must be structured, clear, and follow the specified format exactly.\n","\n","You must organize the output into the following sections:\n","\n","1.  **Executive Summary of Emails**: This is the main title.\n","2.  **Total Number of Emails Received**: Provide the total count of all emails.\n","3.  **Total Number of Emails from Yesterday**: State the total count of emails from the previous day. Note that all emails in this batch are from March 3, 2025.\n","4.  **Email Breakdown by Categories (Count Only)**: List each email category and its corresponding count. The category 'Routine Updates & Check-ins' should be rephrased as 'Routine Updates (Review & Acknowledge)'.\n","5.  **AI Conclusion**: This section should have two distinct sub-sections:\n","    * **Critical Emails Requiring Immediate Attention**: This includes the sum of 'Urgent & High_Priority Emails' and 'Deadline-Driven Emails'. List the counts for each of these categories and then provide a total.\n","    * **Emails That Can Be Reviewed Later**: This includes the sum of 'Routine Updates & Check-ins', 'Non-Urgent Informational Emails', 'Personal & Social Emails', and 'Spam/Unimportant Emails'. List the counts for each of these categories and then provide a total.\n","6.  **Summary Insights**: Write a brief paragraph that highlights the number of critical emails that require immediate attention versus those that can be reviewed later. Emphasize the workload and suggest prioritizing the urgent and deadline-driven items to mitigate risks.\n","\n","Analyze the DataFrame provided in the user prompt to get the counts for each category.\n","\"\"\""],"metadata":{"id":"AQGs5NiqUDEx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title User Prompt\n","\n","user_prompt = f\"\"\"\n","\n","Please generate the executive summary based on the following email data.\n","\n","Below is the attached DataFrame for analysis:\n","\n","\n","Below is the attached DataFrame for analysis:\n","\n","{categorized_df.to_string()}\n","\"\"\""],"metadata":{"id":"q53nfjeqayBl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title  Calling the model and displaying the summary\n","\n","response_1 = llm(system_prompt, user_prompt)        # llm is the model using gpt-4o-mini\n","response_1\n","\n","from IPython.display import display, Markdown\n","display(Markdown(response_1))"],"metadata":{"id":"rhvjMbK7bj6l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Task 1b - Urgent Emails from Yesterday (Must do first today)**"],"metadata":{"id":"leDqwyAuhuDY"}},{"cell_type":"code","source":["# @title System Prompt\n","\n","system_prompt = \"\"\"\n","You are an expert AI assistant tasked with summarizing urgent and high_priority emails.\n","Your goal is to extract key information from each email and present it in a clear, concise, and actionable format.\n","\n","For each email, you must provide the following details:\n","- **Subject**: The subject line of the email.\n","- **Received**: The date the email was received.\n","- **Sender Name**: The name of the sender.\n","- **Summary**: A brief summary of the email's content and its main purpose.\n","- **Next Step**: A specific, actionable step that needs to be taken in response to this particular email.\n","\n","Ensure that the output is formatted as a numbered list, with each email summary clearly delineated.\n","Do not include any introductory or concluding remarks outside of the formatted summaries.\n","\"\"\""],"metadata":{"id":"qq4mpUkWhrrZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Filter out the emails that are urgent and high-priority\n","\n","urgent_emails = categorized_df[categorized_df['category'] == 'Urgent & High_Priority Emails']\n","\n","urgent_df = pd.DataFrame(urgent_emails)"],"metadata":{"id":"XY_p9SQRoAki"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title User Prompt\n","user_prompt = f\"\"\"\n","Below is the attached DataFrame, which contains all the emails that needs to be summarized:\n","\n","{urgent_df.to_string()}\n","\"\"\""],"metadata":{"id":"hkfe_L8VoJcx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title  Calling the model and display the summary\n","\n","response_2 = llm(system_prompt, user_prompt)\n","\n","from IPython.display import display, Markdown\n","display(Markdown(response_2))"],"metadata":{"id":"YeNJLPJ5oWDp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Task 1c - Deadline-Driven Emails from Yesterday (Needs Attention Today)**"],"metadata":{"id":"nrQwRJqCBcpp"}},{"cell_type":"code","source":["# @title System Prompt\n","\n","system_prompt = \"\"\"\n","You are an expert AI assistant tasked with summarizing deadline-driven emails from yesterday.\n","Your goal is to extract key information from each email and present it in a clear, concise, and actionable format.\n","\n","For each email, you must provide the following details:\n","- **Subject**: The subject line of the email.\n","- **Received**: The time and date the email was received.\n","- **Sender Name**: The name of the person who sent the email.\n","- **Summary**: A concise summary of the email's content.\n","- **Next Step**: A specific, actionable step that needs to be taken based on the email, with a clear focus on meeting deadlines. Each 'Next Step' must explicitly mention a specific deadline for today or require action to meet an impending delivery timeline.\n","\n","Ensure that the output is formatted as a numbered list, with each email summary clearly delineated, as follows:\n","\n","1.  Email Exceeding Deadline / Email Requiring Action Today\n","    * **Subject**: (the emails subject line)\n","    * **Received**: (the time and date the email was received)\n","    * **Sender Name**: (the name of the person who sent the email)\n","    * **Summary**: (a concise summary of the emails content)\n","    * **Next Step**: (a specific action that needs to be taken based on the email, with a clear focus on meeting deadlines)\n","\n","After listing all the email summaries, you must provide a final count and a summary of actionable items in the following format:\n","\n","**Final Count of Deadline-Driven Emails**\n","* Total Deadline-Driven Emails: (Count, including a note if any have exceeded their deadline)\n","\n","**Summary of Actionable Items**\n","* (List all the \"Next Step\" actions from the summarized emails as bullet points, combining similar actions if appropriate.)\n","\n","Do not include any introductory or concluding remarks outside of the formatted summaries and the final count/actionable items section.\n","\"\"\""],"metadata":{"id":"92Z3l-vlBnml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Filtering out the emails that are Time Sensitive & Deadline-Driven\n","deadline_emails = categorized_df[categorized_df['category'] == 'Deadline-Driven Emails']\n","\n","deadline_df = pd.DataFrame(deadline_emails)"],"metadata":{"id":"koJ84frxB1GV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title User Prompt\n","\n","\n","# Write your user prompt here\n","user_prompt = f\"\"\"\n","\n","Below is the attached DataFrame, which contains all the emails that needs to be summarized:\n","\n","{deadline_df.to_string()}\n","\n","\"\"\""],"metadata":{"id":"CEQ7p9MGB88U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title  Calling the model and displaying the summary\n","\n","response_3 = llm(system_prompt, user_prompt)\n","\n","from IPython.display import display, Markdown\n","display(Markdown(response_3))"],"metadata":{"id":"JSXrIbwjCoaF","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## **Task 2 - AI Generated \"First Response\" drafts for critical email**"],"metadata":{"id":"WajwulTvV_C9"}},{"cell_type":"code","source":["# @title System Prompt\n","\n","system_prompt = \"\"\"\n","You are an expert AI assistant tasked with drafting professional and contextually relevant first responses to critical emails.\n","Your drafts must acknowledge the sender's request, address key points or queries from the original email, and provide a clear next step or decision.\n","Maintain a polite, formal, and professional tone, adhering to corporate communication standards.\n","\n","For each email, generate a draft in the following format:\n","\n","Subject: (subject of the email)\n","Sender Name: (name of the sender)\n","AI Drafted Reply: (the AI generated response)\n","\n","Do not include any introductory or concluding remarks outside of this specified format.\n","\"\"\""],"metadata":{"id":"iCl0B2t5WL5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Filtering out the emails that are Critical Emails, i.e. ('Urgent & High-Priority Emails' + 'Deadline-Driven Emails')\n","\n","critical_emails = categorized_df[categorized_df['category'].isin(['Urgent & High-Priority Emails', 'Deadline-Driven Emails'])]\n","\n","critical_df = pd.DataFrame(critical_emails)"],"metadata":{"id":"w9u3hyi9W7EX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title User Prompt\n","\n","# You may format the classifying method according to you\n","\n","\n","# Write your user prompt here\n","user_prompt = f\"\"\"\n","Below is a DataFrame containing critical emails for which I need AI-drafted first responses. Please generate a professional draft for each email, following the specified format.\n","\n","{critical_df.to_string()}\n","\"\"\""],"metadata":{"id":"142I2N3KW_hv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title  Calling the model and display the summary\n","\n","response_4 = llm(system_prompt, user_prompt)\n","from IPython.display import display, Markdown\n","display(Markdown(response_4))"],"metadata":{"id":"zTQpevcAXl78"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Task 3 - Evaluation**"],"metadata":{"id":"8yfBElz8actZ"}},{"cell_type":"code","source":["# @title Evaluation System Prompt\n","\n","eval_system_prompt = \"\"\"\n","You are an expert AI Quality Assurance Analyst. Your task is to critically evaluate an AI-generated email draft based on the original email it is responding to. You must assess the draft based on the six key dimensions provided below.\n","\n","Your evaluation must be returned STRICTLY in the specified JSON format. Do not include any introductory text, markdown formatting, or any content outside of the JSON object.\n","\n","**Evaluation Criteria & Scoring:**\n","\n","1.  **Relevance (Score 1-5):** How well does the draft address the core request, questions, and urgency of the original email?\n","    * 1: Poor - Completely misses the point of the original email.\n","    * 3: Average - Addresses the main point but misses key details or context.\n","    * 5: Excellent - Perfectly understands and addresses all key aspects of the original email.\n","\n","2.  **Clarity (Score 1-5):** How clear, concise, and understandable is the language in the draft? Is it free of jargon and ambiguity?\n","    * 1: Poor - Very confusing and poorly worded.\n","    * 3: Average - Generally understandable but could be clearer or more concise.\n","    * 5: Excellent - Exceptionally clear, professional, and easy to understand.\n","\n","3.  **Actionability (Score 1-5):** Does the draft clearly state the next step, decision, or a commitment to action?\n","    * 1: Poor - Vague, non-committal, and provides no clear next step.\n","    * 3: Average - Hints at a next step but lacks specific commitment or timeline.\n","    * 5: Excellent - Provides a clear, specific, and actionable next step.\n","\n","4.  **Strengths:** In one sentence, describe the single biggest strength of the generated draft.\n","\n","5.  **Improvements:** In one sentence, suggest the most important area for improvement.\n","\n","6.  **Overall_Justification:** Provide a 2-3 line summary of your evaluation, justifying the scores given.\n","\n","**Required JSON Output Format:**\n","\n","{\n","  \"Relevance\": {\"score\": 0, \"justification\": \"\"},\n","  \"Clarity\": {\"score\": 0, \"justification\": \"\"},\n","  \"Actionability\": {\"score\": 0, \"justification\": \"\"},\n","  \"Strengths\": \"\",\n","  \"Improvements\": \"\",\n","  \"Overall_Justification\": \"\"\n","}\n","\"\"\""],"metadata":{"id":"YFj6d34uahJe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Evaluation Function & User Prompt\n","\n","def evaluate_summary(eval_system_prompt, eval_user_prompt, eval_model=\"gpt-4o-mini\", temperature=0.0):\n","    try:\n","\n","        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n","\n","        modified_prompt: list[ChatCompletionMessageParam] = [\n","            {'role': 'system', 'content': eval_system_prompt},\n","            {'role': 'user', 'content': eval_user_prompt}\n","            ]\n","\n","        eval_response = client.chat.completions.create(\n","            model=eval_model,\n","            messages=modified_prompt,\n","            temperature=temperature\n","        )\n","\n","        return eval_response.choices[0].message.content\n","\n","    except Exception as e:\n","        error_message = f\"Error evaluating prompt: {e}\"\n","        print(error_message)\n","        return \"{}\"  # Return empty JSON structure on error"],"metadata":{"id":"WukZQkdRNLez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Evaluation for Each Generated Response (Updated)\n","\n","# The response from Task 2 is a single string with drafts separated by \"---\"\n","# We split the string and remove any empty entries that result from the split.\n","evaluation_results = []\n","\n","            # Dynamically create the user prompt with the original email and the generated draft\n","            eval_user_prompt = f\"\"\"\n","            Please evaluate the \"AI-Generated Draft\" based on its response to the \"Original Email\" using the specified JSON format.\n","\n","            **Original Email:**\n","            - **Sender:** {original_email['sender']}\n","            - **Subject:** {original_email['subject']}\n","            - **Body:** {original_email['body']}\n","\n","            ---\n","\n","            **AI-Generated Draft to Evaluate:**\n","            {draft_to_evaluate}\n","            \"\"\"\n","\n","            # Call the evaluation function and append the JSON string to our results list\n","            evaluation_json_str = evaluate_summary(eval_system_prompt, eval_user_prompt)\n","            evaluation_results.append(evaluation_json_str)\n","\n","        except Exception as e:\n","            print(f\"An error occurred during evaluation for email at index {original_email_index}: {e}\")\n","            # Append an empty JSON object on error to avoid breaking the script\n","            evaluation_results.append('{}')\n","\n","print(\"\\\\nEvaluation complete.\")"],"metadata":{"id":"XBYImNQLKiSH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Converting JSON Scores into DataFrame\n","\n","scores = []\n","justifications = []\n","strengths = []\n","improvements = []\n","\n","for result in evaluation_results:\n","    try:\n","        # Load the JSON string into a Python dictionary\n","        result_dict = json.loads(result)\n","\n","        # Extract scores, handling potential missing keys gracefully\n","        score_dict = {\n","            \"Relevance\": result_dict.get(\"Relevance\", {}).get(\"score\", None),\n","            \"Clarity\": result_dict.get(\"Clarity\", {}).get(\"score\", None),\n","            \"Actionability\": result_dict.get(\"Actionability\", {}).get(\"score\", None)\n","        }\n","        justification = result_dict.get(\"Overall_Justification\", \"NA\")\n","        strength = result_dict.get(\"Strengths\", \"NA\")\n","        improvement = result_dict.get(\"Improvements\", \"NA\")\n","\n","        # Append the extracted data to our lists\n","        scores.append(score_dict)\n","        justifications.append(justification)\n","        strengths.append(strength)\n","        improvements.append(improvement)\n","\n","    except (json.JSONDecodeError, AttributeError) as e:\n","        print(f\"Error parsing a result: {e}\\\\nResult string: {result}\")\n","        # Append placeholder data if a result is malformed\n","        scores.append({\"Relevance\": None, \"Clarity\": None, \"Actionability\": None})\n","        justifications.append(\"Error parsing result\")\n","        strengths.append(\"Error parsing result\")\n","        improvements.append(\"Error parsing result\")\n","\n","# Create the final DataFrame\n","pd.set_option('display.max_colwidth', None)\n","df_scores = pd.DataFrame(scores)\n","df_scores[\"Strengths\"] = strengths\n","df_scores[\"Improvements\"] = improvements\n","df_scores[\"Justification\"] = justifications"],"metadata":{"id":"4W9YWpjTKriO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Display Final Evaluation Table\n","display(df_scores)"],"metadata":{"id":"YkBVOB0ZK2O3"},"execution_count":null,"outputs":[]}]}