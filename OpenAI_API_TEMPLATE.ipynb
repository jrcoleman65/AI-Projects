{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1dZqvCIEPohvGAzcUPb7XCEUfEbBS5bvq","timestamp":1755879723268}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### ⚡  **USING OPENAI API TEMPLATE** ⚡\n","\n","---"],"metadata":{"id":"5QIPQX8gm4g8"}},{"cell_type":"markdown","source":["### **STANDARD PROMPTING**"],"metadata":{"id":"hXfa1KElnINx"}},{"cell_type":"code","source":["# @title Install Libaries\n","\n","! install openai\n","! install IPython"],"metadata":{"id":"lBNaK344l7wc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Imports\n","\n","from openai import OpenAI\n","import json, os\n","from IPython.display import Markdown, display\n","from openai.types.chat import ChatCompletionMessageParam"],"metadata":{"id":"DIAVJMW9nsED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Supress Warnings (OPTIONAL)\n","\n","#import warnings\n","#warnings.filterwarnings('ignore')"],"metadata":{"id":"hqQf2ra7o_US"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Mount Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"id":"oQKDNVmCnztD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title CD to Project Directory\n","\n","# Change the directory to where your project files are located\n","%cd /content/drive/MyDrive/GenAI/OpenAI/OpenAI_API_Introduction"],"metadata":{"id":"ZD1TK6GzrESJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Load API Key\n","\n","file_name = '/content/drive/MyDrive/FOLDER/config.json'\n","\n","with open(file_name, 'r') as file:\n","  config = json.load(file)\n","  os.environ['OPENAI_API_KEY'] = config.get(\"API_KEY\")\n","  os.environ[\"OPENAI_BASE_URL\"] = config.get(\"OPENAI_API_BASE\")"],"metadata":{"id":"IjRBH21noEly"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Create System Prompt\n","\n","system_prompt = \"\"\"\n","\n","<System prompt goes here>\n","\n","\"\"\""],"metadata":{"id":"TijuWy-8p_7I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Create User Prompt\n","\n","user_prompt = \"\"\"\n","\n","<User prompt goes here>\n","\n","\"\"\""],"metadata":{"id":"aoOw0JPGqdux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Create LLM Function\n","\n","# LLM Model you want to use\n","model_name = \"gpt-4o-mini\"\n","\n","# temp - between 0-2, Higher more random, lower more focused.  Use temp or top_p but not both\n","temperature = 0.5\n","\n","# Presence_penalty between -2.0 and 2.0. Increases the model's likelihood to talk about new topics\n","presence_penalty = 0\n","\n","# frequency_penalty - between -2.0 and 2.0.  Positive values penalize new tokens based on exiting frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n","frequency_penalty = 0\n","\n","# max tokens\n","max_tokens = 2048\n","\n","# response format\n","response_format = {\"type\": \"text\"}\n","\n","def llm(system_prompt,\n","        user_prompt,\n","        model_name=model_name,\n","        temperature=temperature,\n","        presence_penalty = presence_penalty,\n","        frequency_penalty = frequency_penalty,\n","        max_tokens = max_tokens,\n","        response_format =  response_format\n","        ):\n","\n","    try:\n","\n","        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n","\n","        prompt: list[ChatCompletionMessageParam] = [\n","            {'role': 'system', 'content': system_prompt},\n","            {'role': 'user', 'content': user_prompt}\n","        ]\n","\n","        # if you use any other parameters outside their defaults, you need to include them here\n","        response = client.chat.completions.create(\n","            model=model_name,\n","            messages=prompt,\n","            temperature=temperature\n","        )\n","\n","        return response.choices[0].message.content\n","\n","    except Exception as e:\n","        error_message = f\"Sorry, I encountered the following error: {e}\"\n","        print(error_message)\n","        return error_message"],"metadata":{"id":"QXI_3Lz-oWyS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Calling the LLM Function - Assigning Value to Response Variable\n","\n","response = llm(system_prompt, user_prompt)\n","response"],"metadata":{"id":"_K4yg67wojia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Displaying the Response\n","\n","from IPython.display import display, Markdown\n","display(Markdown(response))"],"metadata":{"id":"sgizLJH6ppjh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **IMAGE TO TEXT**"],"metadata":{"id":"5Shc4osnmnNx"}},{"cell_type":"code","source":["# @title URL of an image we will use later\n","url = \"https://upload.wikimedia.org/wikipedia/commons/7/71/Cow_and_calf_elk_%287437504452%29.jpg\""],"metadata":{"id":"ELA5Ls58muY5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Create Image System Prompt\n","\n","image_system_prompt = \"\"\"\n","\n","\"\"\""],"metadata":{"id":"lL3g3CHqmzIJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Create Image User Prompt\n","\n","image_user_prompt = \"\"\"\n","\n","user_content = [\n","    {\"type\": \"text\",\n","     \"text\": \"Describe the image\"},\n","    {\"type\": \"image_url\",\n","     \"image_url\": {\n","         \"url\": url,\n","         \"detail\":\"high\"\n","     }}\n","]\n","\n","\"\"\""],"metadata":{"id":"kjQl58FRm1m5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Create IMAGES LLM Function\n","\n","# LLM Model you want to use\n","model_name = \"gpt-4o-mini\"\n","\n","# temp - between 0-2, Higher more random, lower more focused.  Use temp or top_p but not both\n","temperature = 0.5\n","\n","# Presence_penalty between -2.0 and 2.0. Increases the model's likelihood to talk about new topics\n","presence_penalty = 0\n","\n","# frequency_penalty - between -2.0 and 2.0.  Positive values penalize new tokens based on exiting frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n","frequency_penalty = 0\n","\n","# max tokens\n","max_tokens = 2048\n","\n","# response format\n","response_format = {\"type\": \"text\"}\n","\n","def image_llm(system_prompt,\n","        user_prompt,\n","        model_name=model_name,\n","        temperature=temperature,\n","        presence_penalty = presence_penalty,\n","        frequency_penalty = frequency_penalty,\n","        max_tokens = max_tokens,\n","        response_format =  response_format\n","        ):\n","\n","    try:\n","\n","        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n","\n","        prompt: list[ChatCompletionMessageParam] = [\n","            {'role': 'system', 'content': system_prompt},\n","            {'role': 'user', 'content': user_prompt}\n","        ]\n","\n","        # if you use any other parameters outside their defaults, you need to include them here\n","        response = client.chat.completions.create(\n","            model=model_name,\n","            messages = [\n","                {\"role\": \"user\", \"content\": user_content}\n","            ],\n","            temperature=temperature\n","        )\n","\n","        return response.choices[0].message.content\n","\n","    except Exception as e:\n","        error_message = f\"Sorry, I encountered the following error: {e}\"\n","        print(error_message)\n","        return error_message"],"metadata":{"id":"NJN5TWJFm4KR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Using chat.completions\n","\n","response = image_llm(image_system_prompt, image_user_prompt)"],"metadata":{"id":"9UpDHtL0m9gg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Displaying the Response\n","\n","from IPython.display import display, Markdown\n","display(Markdown(response))"],"metadata":{"id":"gZCAkjyDm_s4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **CONVERTING IMAGES & PDFS TO EXCEL SPREADSHEET**"],"metadata":{"id":"Q__ocKzK28Qh"}}]}